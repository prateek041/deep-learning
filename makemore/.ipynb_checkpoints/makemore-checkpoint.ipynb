{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82306105-11af-4a56-bfa5-0301420a0aec",
   "metadata": {},
   "source": [
    "# What we are building\n",
    "\n",
    "We are building a language model, that can predict the next character in a sequece, by looking at the character just before it.\n",
    "This is called **Character Level** Language Model.\n",
    "\n",
    "There are multiple ways of implementing this, some of the progressively better methods are listed below:\n",
    "- N-gram (Since we are using two character pairs, it is called Bigram)\n",
    "- MLP (Multi layer perceptron)\n",
    "etc.\n",
    "\n",
    "The outcome will be a model, that gets better at **generating** name like words, since our training data itself is a long list\n",
    "of names scraped from the internet.\n",
    "\n",
    "Let's get started with the first Approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f80ee-a478-47bb-add2-bdaabdd3c109",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06aca7-afd4-4201-8b1a-450c60d1da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines() # reading the names from \"names.txt\".\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e8373-1f9b-4924-9f11-c4fa4252f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd3efd-c7b6-4312-a6ea-80de287592ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(word)for word in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eccc532-65e0-4453-8643-d1dc4b8dfb83",
   "metadata": {},
   "source": [
    "# N-Gram Approach in Language Modelling\n",
    "\n",
    "In language modelling, **N-Gram** is an early approach based purely on **statistics**. It is based on the approach of predicting the next word, depending on \"how many times has this sequence occured in the training set\".\n",
    "\n",
    "So, let's say we have **counts** of how many times a certain sequence of words appeared in the training data, it would be something\n",
    "like:\n",
    "- [my, name]: 100\n",
    "- [name, is]: 132\n",
    "- [is, prateek]: 21\n",
    "- [my, potato]: 1\n",
    "...\n",
    "\n",
    "This says that the combination of the words \"my\" and \"name\" occured 100 times in our training data, while the combination of words \"potato\" and \"umbrella\" appeared just once.\n",
    "\n",
    "So, when the model comes across the word \"my\" and has to predict the next word, since **statistically** the combination of\n",
    "\"my\" and \"name\" has occured more, it has higher probability and hence, the model is **more likely** to predict the word\n",
    "\"name\" as the next word instead of \"potato\", when it sees \"my\".\n",
    "\n",
    "If the model is working on pair of 2, it is called a **Bigram** model, similarly a pair of 3 will be a **Trigram** model.\n",
    "\n",
    "One important thing to understand here is that, **There is no learning** involved here, like we would expect in case of\n",
    "a **Neural Network** based model, it is just mathematical representation of training data, and therefore there are huge\n",
    "problems with this approach, which will be discussed throughout this article, but a brief summary would be:\n",
    "\n",
    "- Lack of **Generalisation**: This model does not perform well on **unseen** data, we can do tricks like **model smoothing** (discussed below), but these are just workarounds, because statistically, if the combination never occured in the training set, it will have almost negligible probability.\n",
    "- Does not Scale: As discussed earlier, this model assigns *counts* to all the possible combinations, and how many times they occured in the training set. If we move from bigram to trigram, the possible combinations exponentially grow.\n",
    "\n",
    "\n",
    "## How to train them\n",
    "\n",
    "To make things easier to understand, let's decide on training a Bigram, and instead of looking at words, we are working on a \n",
    "**Character level Bigram model**, which predicts the next character, just by looking at the previous character.\n",
    "\n",
    "This is the list of Broadly defined steps we will take, to train the bigram model, each step is well described in it's section.\n",
    "- Preparing the data.\n",
    "- Creating Bigrams and calculating counts.\n",
    "- **Normalising** to assign probabilities\n",
    "- Model smoothing to prevent -inf probabilities\n",
    "\n",
    "First, we have to create **bigrams** from our training data, that is done by:\n",
    "- starting at the beginning of our training data.\n",
    "- creating character pairs from each word in the training data.\n",
    "\n",
    "So, if the first name is **emma**, the bigrams we get from that are:\n",
    "- em\n",
    "- mm\n",
    "- ma\n",
    "\n",
    "but this is not complete, there is no way for model to know, what is the starting/ending of a word. We need introduce something\n",
    "from our end into the data.\n",
    "\n",
    "Let's say we decide to enclose each word in \"*\", so `emma` becomes `*emma*`. Through this, the bigrams become\n",
    "- *e (representing the combination where `e` is the beginning of a word)\n",
    "- em\n",
    "- mm\n",
    "- ma\n",
    "- a* (representing the combination where `a` is the end of a word)\n",
    "\n",
    "\n",
    "\n",
    "We start by deciding the length of word combination i.e. deciding if we are training a bigram, trigram or so on. One important\n",
    "thing to understand here is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e985d-68df-4c46-b441-463bf681053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {} # B is the bigram, we are making a huge object, where key is the character pairs and value is their count.\n",
    "for w in words:\n",
    "    # halucinate a special start character\n",
    "    chs = ['<S>'] + list(w) + ['<E>'] # converting all the words into arrays, starting with <S> and ending with <E>\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # There is a great explanation for what zip does and how it works. But we are just mapping words in pair.\n",
    "        entry = (ch1, ch2) # pairing up characters\n",
    "        b[entry] = b.get(entry, 0) + 1  # entering them in the dictionary with their count of occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708afd17-eee2-4f51-a945-aacabbe21077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are trying to sort the items in our bigram.\n",
    "# since it is a dictionary, we cannot directly use sorted here.\n",
    "# below, b.items() converts our bigram into an array of touples, where each touple contains two items.\n",
    "# the first item is the key, which in our case is character pair, and the second item is it's count.\n",
    "# Then, for functions like sorted, min, max etc. we can use key, to tell what value the function\n",
    "# should use to sort things, in our case, we want to sort using occurences of a character pair, i.e.\n",
    "# the second item of the touple.\n",
    "# We also want it in descending order, therefore before returning the count to the key parameter, we\n",
    "# negate it, therefore making the biggest value, the smallest and vice verca. Hence, sorted returned\n",
    "# us a reverse sorted array of touples, sorted on the basis of occurence of each character pair.\n",
    "sorted(b.items(), key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81c4b8-38e5-4e6c-a8ba-1b83356e83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to convert the data from dictionary to pytorch tensors for faster manipulation\n",
    "# and all the goodness it brings with it.\n",
    "# Here the rows and columns represent the characters, and each entry itself\n",
    "# is a number. So, each entry is a unique combination of 28 characters and\n",
    "# tells how many times that unique combination appeared.\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39dbef-e5e8-4243-b89a-673513019d3d",
   "metadata": {},
   "source": [
    "# N-gram Approach (Bi-gram)\n",
    "\n",
    "We will start with building a character level, Bigram Model.\n",
    "> !NOTE:\n",
    "> Add more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30f0a7-c6b2-4a1c-972c-254207fbe6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have 26 letters in the alphabet + 2 special characters for \n",
    "# start and end, we have overall 28 characters in our set.\n",
    "# This creates a 2D tensor, of 28 x 28 dimension so that we can represent corrences of each pair through numeric values\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d18b9-c9e6-4047-8894-a1e54015d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to fill up N with our characters just like \"b\" above.\n",
    "# But we cannot directly store our characters into N because\n",
    "# we have characters as the keys if you remember our b dictionary.\n",
    "# Therefore we need a sort of a lookup table to map \n",
    "# the characters in the int32 type torch tensor.\n",
    "\n",
    "# First get all the unique characters in the names.txt file.\n",
    "# This is done in three steps.\n",
    "# \"\".join(words) creates a long list of all the words in our data. \"emmaolivia...\"\n",
    "# set(), following it's property, gets all the unique characters in that huge list. \"abcdefghij...\"\n",
    "# list(), converts the set into a list.[\"a\", \"f\", \"c\", ...]\n",
    "# sorted(), by default sorts things in an ascending order. ['a', 'b', 'c', ...]\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56589c53-554e-4944-bef8-09ec71d45c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate converts an array into an enum, so we can loop over it using index and value.\n",
    "# the below code is creating a dictionary, where each key is characters in english, and each value is a numeric value assigned to them.\n",
    "# which in our case is their index value in ascending order.\n",
    "# we also want to add one more character, \".\" and assigning it index value \"0\".\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dd5cb-ee98-4cf8-8519-e33fd8f5c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"] # insteading of using <S> for start and <E> for end, we are using \".\" to signify both.\n",
    "    for ch1, ch2 in zip(chs, chs[1:]): # creating touples of word pairs from each word in our data, therefore mapping every possible combination.\n",
    "        xi = stoi[ch1] # The numeric value of the first character in our character pair.\n",
    "        xj = stoi[ch2] # The numeric value of the second character in our character pair.\n",
    "        N[xi, xj] += 1 # Putting the numeric value in our tensor\n",
    "        \n",
    "# Now, N is a 2D array, where each entry represents the number of occurences of a character pair, but since rows and columns of a tensor\n",
    "# are indexes in an array and cannot be a character itself, we used the map we created in the previous step. Therefore instead of representing\n",
    "# (a, b) occuring 10 times, the tensor holds 10 at the index (1, 2) because a -> 1 and b -> 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cb243-709e-47b6-ad42-b98bada00753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make it appear more beautiful, to do this we will use matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# show matplot lib charts inline.\n",
    "\n",
    "itos = {i: s for s, i in stoi.items()} # does the exact same things as above, just flipping keys with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898aa76-2fbb-4db3-ab1a-5b0f737c27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "for i in range (27):\n",
    "    for j in range (27):\n",
    "        chrstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chrstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a15ca-50ba-424d-b851-cd325491a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next phase is about sampling, for which we cannot use raw counts, of \n",
    "# how many times a character combination appeared, we need probabilities.\n",
    "\n",
    "# We start by sampling the first character, which is the character comoing right after the \".\".\n",
    "# Probablity of a given output, out of a number of outputs is x/sum, where sum is addition of all\n",
    "# probabilities.\n",
    "\n",
    "# In order to get the probability of a character, occuring at the beginning of a name, we will do the following steps.\n",
    "# Get the first row from our tensor, which tells us the count of all the characters coming first.\n",
    "# Then, we convert them in float because we are going to be dividing things.\n",
    "# Then, we get the sum of all possibilities. Which is, adding up each occurence, of every character in our dataset.\n",
    "# which in our case is just adding every entry in our first row.\n",
    "# Then, we divide each individual charcacter-occurence-count with that total, giving us probability of that specific \n",
    "# count, where if we add each probability, we get 1 as the sum total.\n",
    "# This is sampling.\n",
    "\n",
    "count = N[0].float()\n",
    "count /= count.sum()\n",
    "\n",
    "# To sample, we are going to use torch.multinomial. This will take a probability\n",
    "# distribution and give out integers to us.\n",
    "# Now, since we want the system to be little deterministic, we are going to provide\n",
    "# a generator to it, so it produces same outputs irrespective of the system running\n",
    "# it.\n",
    "\n",
    "# generator that makes sure that the random numbers generated are same all the time.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# generate 3 random number between 0 and 1.\n",
    "p = torch.rand(3, generator=g) # generator object is the source of randomness.\n",
    "# Normalising it to generate probabilities.\n",
    "p = p/p.sum() # probability distribution of the generated random numbers.\n",
    "# use probabilities to generate numbers\n",
    "num = torch.multinomial(p, num_samples=10, generator=g, replacement=True)\n",
    "\n",
    "# So this can all be sum down to, we have probabilities, using that probability distribution.\n",
    "# we need a number. The probability of that number being thrown out depends\n",
    "# on the probability we provided to the multinomial.\n",
    "# In real world, our probability distribution is going to come from our data and not torch.rand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314b0bc-7a8c-4de8-baee-99dfec6ffe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[0].float()\n",
    "p = p/p.sum()\n",
    "\n",
    "ix = torch.multinomial(p, num_samples=1, generator=g, replacement=True).item()\n",
    "itos[ix]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d44d4a-4639-431c-b6f6-99ab3499cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the sampling loop now.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(20): # We want to generate 20 words.\n",
    "    ix = 0\n",
    "    word = []\n",
    "    while True:\n",
    "        p = N[ix].float() # get ith row.\n",
    "        p = p / p.sum() # generating probability distribution of that row.\n",
    "        ix = torch.multinomial(p, num_samples=1, generator=g, replacement=True).item() # get 1 integer, whose probability of appearing depends on the probability distribution.\n",
    "        character = itos[ix] # character mapping of that generated integer index.\n",
    "        word.append(character) # add it to the previously generated word.\n",
    "        if ix == 0:\n",
    "            break # break if reached the end \".\"\n",
    "    print(\"\".join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70941b26-2ac7-44c5-8060-3ec45374b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above implementation is quite in-efficient. Simply because we are converting each row into a float and then\n",
    "# generating probability distribution for each row, which means things happen 20 times, if we generate 20 words.\n",
    "# Therefore, we will compute once, the probabilities into a P Tensor.\n",
    "\n",
    "# First thing we want is, sum of each row. Which should give us a row, where each item represents sum of all the items in that\n",
    "# row of our 27x27 matrix. We use torch.sum for that\n",
    "\n",
    "P = N.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74413dbd-21ef-40c3-bc7d-a889bd02093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P.sum() gives the sum of the entire matrix\n",
    "P = P / P.sum(1, keepdim=True) # doing it across 0 will sum things up around column (top to bottom), but we want left to right.\n",
    "\n",
    "# We can do the above operation because broadcasting semantics allow this.\n",
    "# Above we are dividing 27 x 27 matrix, with a 27 x 1 column matrix.\n",
    "# Since it is allowed, while division, 27x1 matrix is converted into a 27 x 27 matrix and then the division\n",
    "# operation is performed, hence createing a matrix, which is probability distribution of our data.\n",
    "\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f15ea4-0d4e-40d3-9f14-b5115612a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can optimise our word generation logic a little.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(20): # We want to generate 20 words.\n",
    "    ix = 0 # first word is \".\" since it is the starting character.\n",
    "    word = []\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, generator=g, replacement=True).item() # get 1 integer, whose probability of appearing depends on the probability distribution.\n",
    "        character = itos[ix] # character mapping of that generated integer index.\n",
    "        word.append(character) # add it to the previously generated word.\n",
    "        if ix == 0:\n",
    "            break # break if reached the end \".\"\n",
    "    print(\"\".join(word))\n",
    "# Output still remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d020f1-3c09-4290-bbcd-8e340779d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "for w in words[:3]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        log_prob = torch.log(prob)\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {log_prob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef777c7-e67d-4c9a-94fb-ba692bce20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to evaluate the quality of this modelling.\n",
    "# An untrained model will have 1/27 probability of anything coming.\n",
    "# We can say that the model learnt something when the probability of something coming up is more than 1/27.\n",
    "# How can we sumarise the probabilities, into a single number, that helps us understanding the accuracy of the model.\n",
    "# Maximum Likelihood Estimation, statistical modelling. (Likelihood). product of these probabilities should be very high.\n",
    "# since the probabilities we are dealing with are very small numbers, therefore their  product will be even smaller,\n",
    "# Therefore instead of dealing directly with probabilities, people work with logarithms of these. (Log Likelihood).\n",
    "\n",
    "# The job of our training is to find the parameters to reduce the negative logarithmic loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772eafbc-889e-402d-af33-ad21c8ac3b2a",
   "metadata": {},
   "source": [
    "## Concluding notes\n",
    "\n",
    "There is more to look into for\n",
    "- Learn how Matplotlib works (Good to have)\n",
    "- Get deeper into sampling, come up with a good example (Core)\n",
    "  - Probablity\n",
    "  - Normalisation\n",
    "  - Distribution\n",
    "- Learn broadcasting and tensor manipulation. (Core)\n",
    "- Likelihood, model parameters, statistical modelling. (Core)\n",
    "- log likelihood.\n",
    "- negative log likelihood.\n",
    "- average negative log likelihood.\n",
    "- Model smoothing.(Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2790b38-89a9-4823-839d-9fe1edd799c2",
   "metadata": {},
   "source": [
    "# Quality of the Model\n",
    "\n",
    "The measure of how good is the model at **predicting the training set**.\n",
    "\n",
    "This is the question that all the training boils down to. To measure how well a Language model is doing, we use\n",
    "a term called **Perplexity**. Which means **How surprised was the model with our test?**. We want the model to be less surprised.\n",
    "\n",
    "When training a model, we split our entire data into mostly two splits, **Training** and **Test**. Training set\n",
    "is what the model learns from, and the test set is completely unseen by the model, and it evaluates the **goodness** of it.\n",
    "\n",
    "In our **Character Level Bigram Model**, we saw that each character combination has a **Probability** assigned to it which we got after **Normalising** the Tensor. These probabilities are calculated through **Maximum Likelihood Estimation** since we created a `27x27` grid that represented the number of times each character combination appeared.\n",
    "\n",
    "A completely untrained model would have almost equal probabilities (1/27 since we have 27 unique characters) for every character combination, which shows that our model doesn't understand patterns yet, but after normalising, we saw that probability distribution changed.\n",
    "\n",
    "### MLE (Maximum Lilkelihood Estimation)\n",
    "\n",
    "MLE is **counting** how often a pair of words/characters appear in your training data and using those counts\n",
    "to guess probabilities.\n",
    "\n",
    "### Likelihood\n",
    "It is the product of the entire probability distribution, it tells how well model's *probability distribution* matches the real data. Likelihood should be high.\n",
    "\n",
    "## Evaluating\n",
    "We use our test set to evaluate how good our model mimics training data. Let's take an example first.\n",
    "\n",
    "Out of all the names in `names.txt`, let's pick emma. It's bigrams are `.->e`, `e->m`, `m->m` and`m->a`. Let's check if our model can \"guess\" the sentence correctly.\n",
    "\n",
    "From our training data, the bigram probabilities are:\n",
    "- P(\".\"|\"e\") = 0.0478 (probability of e as a starting character)\n",
    "- P(\"e\"|\"m\") = 0.0377 (probabilty of m coming after e)\n",
    "- P(\"m\"|\"m\") = 0.0253 (probability of m coming after m)\n",
    "- P(\"m\"|\"a\") = 0.3899(probability of a coming after m)\n",
    "\n",
    "So, **Likelihood** becomes `0.0478 * 0.0377 * 0.0253 * 0.3899` which is `0.00001777636681`. This number is quite small, but we want it to be high, so that the **perplexity** of the model is low. The next few steps are all about dealing with that.\n",
    "\n",
    "### Log Likelihood\n",
    "We all know the mathematical rule, `log(a*b*c) = log(a) + log(b) + log(c)`. This prevents multiplying decimal numbers, since multipying two decimal number leads to even smaller number.\n",
    "\n",
    "-3.0408\n",
    "em: 0.0377 -3.2793\n",
    "mm: 0.0253 -3.6772\n",
    "ma: 0.3899 -0.9418\n",
    "Which turns out to be `-3.0408 + -3.2793 + -3.6772 + -0.9418` which turns out to be `-10.9391`. Now, since in the previous step we wanted to \"maximise\" Likelihood, we must maximise this Log as well, but if we look at the [**graph of log**](https://www.wolframalpha.com/input?i=log%28x%29+from+0+to+1), it looks like this.\n",
    "\n",
    "The values are ranging from `- infinity` (minimum) to `0` (maximum). We are taking log from 0 to 1 because the probability of anything is bound between 0 and 1.\n",
    "\n",
    "So, according to Log Likelihood, the highest quality model will give Log Likelihood very close to 0.\n",
    "\n",
    "But, in the AI world, we talk about **Loss Function**, which tells us how differnt is the **Predicted** outcome, from the **Expected Output**. We want that difference to be as low as possible, hence the term, **minimising the loss function**. Let's come up with a loss function for our problem.\n",
    "\n",
    "### Negative Log Likelihood (NLL)\n",
    "\n",
    "Just multiply **Log Likelihood** by -1, *maximizing* Log Likelihood is equivalent to *minimizing* Negative Log Likelihood.\n",
    "\n",
    "## Conclusing\n",
    "\n",
    "The NLL for our training set, is <value here> Quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5bd6bd-1d40-4eb4-8bd5-f46aa59077f2",
   "metadata": {},
   "source": [
    "# Neural Network Way\n",
    "\n",
    "Takes a character as input, runs through the neural network, gives the next token. Now, we also get to tweak the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d617bdf6-f808-4a56-be53-4e696d07e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n",
      "tensor([ 0,  5, 13, 13,  1])\n",
      "tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# Create the training set of bigrams [input, output]\n",
    "\n",
    "inputs, outputs = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        inputs.append(ix1)\n",
    "        outputs.append(ix2)\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "outputs = torch.tensor(outputs)\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144177a3-bfc7-4ab6-9586-afca6934fd1e",
   "metadata": {},
   "source": [
    "## Thought process\n",
    "\n",
    "We need to tweak the weights of the neural network so that when the input is 0, the probability of 5 as an output should be the highest.\n",
    "\n",
    "Next we need to feed the inputs into the neural network, and we can't just directly input the character indexes into it.\n",
    "\n",
    "## One Hot encoding\n",
    "\n",
    "It is the process of representing input as arrays of bits, if we have 27 different inputs, we create an array of 27x1 where each only one of those 27 bits can be 1, hence representing an input.\n",
    "\n",
    "Then we can take any size of matrix as input, pytorch will calculate things in parallel.\n",
    "\n",
    "We are trying to find **Firing Rate** for a neuron, depending on the input. 27x27 means 27 neurons, where one neuron is a column vector of random integers which are weights.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f76296-068e-43ea-a101-89f42955b359",
   "metadata": {},
   "source": [
    "We have touched upon multiple topics but here is what I understood.\n",
    "\n",
    "- We have 27 characers with which we want to work with.\n",
    "- We need to convert them into numbers, because models don't understand english.\n",
    "- We don't need them as integers in the model because everything inside neural network needs to be a floating point value.(I am confused with this)\n",
    "- So, we create tensor vectors using Hot Encoding as explained above.\n",
    "- Then we can batch multiple inputs together to run through the neuroal network. One input would be a row vector.\n",
    "- First step is to create neurons, we create them by just assigning random numbers in a column vector.\n",
    "- We want to check the fire rate of these neurons in terms of every input, therefore we have 27 neurons in the layer. 27x27 matrix. (I am confused here, like why 27 neurons? why not just one?)\n",
    "- Then we calculate dot products.\n",
    "- Dot product with random numbers turns out to random floating point numbers positive and negative.\n",
    "- We need positive numbers, because we want something that can be used to represent counts? I don't know.\n",
    "- So we consider the output of these dot product to produce logits (logs of actual counts).\n",
    "- Then we use exponent to get the actual count out.\n",
    "- Then we normalise them using the same old method of calcuating probability distribution.\n",
    "- At this point, we have 5x27 matrix as output, where each row represents a probability vector, where each individual probability is the probability of 1 our ot 27 characters to be.\n",
    "- As the model learns these probabilities change to make the predictions better.\n",
    "- We change weights through Back propogation.\n",
    "\n",
    "We are using NLL instead of mean squared error because current problem is classification and not regressive.\n",
    "\n",
    "Smoothing and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c8e4f-64ed-40eb-be8a-7ebb12161122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
